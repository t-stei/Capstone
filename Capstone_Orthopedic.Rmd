---
title: "Capstone Orthopedic"
author: "Til Stein"
date: "13/05/2020"
output:
  pdf_document: default
  html_document: default
---
```{r Package installation and data download, include = FALSE}
#Installing required packages
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(matrixStats)) install.packages("matrixStats", repos = "http://cran.us.r-project.org")
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org")
if(!require(ranger)) install.packages("ranger", repos = "http://cran.us.r-project.org")
if(!require(ordinalForest)) install.packages("ordinalForest", repos = "http://cran.us.r-project.org")
if(!require(kernlab)) install.packages("kernlab", repos = "http://cran.us.r-project.org")
if(!require(ggpubr)) install.packages("ggpubr", repos = "http://cran.us.r-project.org")
if(!require(tibble)) install.packages("ggpubr", repos = "http://cran.us.r-project.org")

#Download URL
urlfile="https://raw.githubusercontent.com/t-stei/Capstone/master/column_2C_weka.csv"
#Create Data
patients<-read_csv(url(urlfile))
```

```{r partitioning the data, include = FALSE, warning = FALSE}
#Setting the seed
set.seed(1, sample.kind = "Rounding")
#Choosing test index
test_index <- createDataPartition(patients$class, times = 1, p = 0.2, list = FALSE)
test_index <- as.vector(test_index)
#Creating test_set
test_set <- patients[test_index, ]
#Creating train set
train_set <- patients[-test_index, ]

```

```{r README, include = FALSE}
####
# I disabled warnings for all chunks where I applied the sample.kind = "Rounding" argument to the set.seed function. Feel free to activate it, if you want to check for warnings.

# This project was written on R 3.6.3
####
```


## 1. Introduction

In this analysis, I aim to find abnormalities in the spine from patient data such as the sacral slope or the pelvic tilt. I will start with single variable methods, before I move on to multivariate and ensemble analysis. 

## 1.1 Data Source
The data I am using stems from the website kaggle. Here is the link:
https://www.kaggle.com/uciml/biomechanical-features-of-orthopedic-patients/data?select=column_2C_weka.csv

They originally downloaded it from the UCI ML repository. Credits go to:
Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science

## 1.2 Data Splitting

For this analysis, I randomly partitioned the data into a training set and a test set. I will use the training set to build my algorith and the test set only at the end, to test it. Therefore, all analysis of the data refers to the training set. Of the 310 patients, 248 (80%) are in the training set and 62 (20%) are in the test set. This is a very common way of splitting the data, as it gives me a good amount of entries to train my algorithm, as well as sufficient patients to test my algorithm. A split of 90/10 would have left me with only 31 patients to verify my finding, which could lead to unstable end results.

### 1.3 Data Insight
#### 1.3.1 Prediction Variable
In the data, orthopedic patients are classified as "normal" or "abnormal". Patients that are classified as "abnormal" either have a disk hernia or a spondylolisthesis. Patients that are classified as "Normal", have a healthy spine. Within the data are body measurements of the patients. These body measurements are: pelvic incidence, pelvic tilt, lumbar lordosis angle, sacral slope, pelvic radius and degree of spondylolisthesis. In the first plot we can see that there are more patients with abnormalities (168) than healthy patients (80). This also expalins, why I haven't divided the set into a training, testing and validation set.

```{r distribution of class, echo  = FALSE}
#Creating the Plot
class_dis <- train_set %>% ggplot(aes(class, fill="red")) + geom_bar()+ 
  theme(legend.position = "none") + labs(title = "Distribution of Class") +
  geom_text(stat='count', aes(label=..count..), vjust=-1) + xlab("Health Status") + ylab("Number of Patients")
#Expanding Y Limits (Relative to Graph)
ylim <- ggplot_build(class_dis)$layout$panel_scales_y[[1]]$range$range
exp_ylim <- ylim[2]+14
class_dis <- class_dis + expand_limits(y =c(NA,exp_ylim))
#Printing Plot
class_dis

```

#### 1.3.2 Predictors
```{r distribution of continuous variables, echo = FALSE}
#Creating Plots that show distribution of continuos variables
p1 <-train_set %>% ggplot(aes(x = pelvic_incidence, fill = class)) +          geom_density(aes(y=..density..))
p2 <-train_set %>% ggplot(aes(x = `pelvic_tilt numeric`, fill = class)) +          geom_density(aes(y=..density..))
p3 <-train_set %>% ggplot(aes(x = lumbar_lordosis_angle, fill = class)) +          geom_density(aes(y=..density..))
p4 <-train_set %>% ggplot(aes(x = sacral_slope, fill = class)) +          geom_density(aes(y=..density..))
p5 <-train_set %>% ggplot(aes(x = pelvic_radius, fill = class)) +          geom_density(aes(y=..density..))
p6 <-train_set %>% ggplot(aes(x = degree_spondylolisthesis, fill = class)) +          geom_density(aes(y=..density..))
#Creating Grid
ggarrange(p1,p2,p3,p4,p5,p6,nrow = 2, ncol=3, common.legend = TRUE, legend = "bottom")

```

Above, we can see the distribution of our predictors. Furthermore, the classes are encoded by colour. We can see that most variables are somewhat normally distributed. However, the degree of spondylolisthesis predictor has most of its data at around 0, with extreme offshoots.

```{r Preprocessing - checking for variable variability, echo = FALSE}
#Calculating SD of Variables
sd_col <- colSds(as.matrix(train_set[,1:6]))
#Creating a variable names vector
names <- colnames(train_set)[1:6]
#Creating a new data frame to store names and sd of cols
var_df <- data.frame(names, sd_col)
#Plotting sd of collumns
var_df %>% ggplot(aes(x = names, y = sd_col)) + geom_bar(stat= "identity") + theme(axis.text.x = element_text(angle = 90)) + xlab("Variable") + ylab("Standard Deviation") + labs(title = "Standard Deviation of Variables")

```

If we look at the variance within our predictors, we can see that degree of spondylolisthesis has the highest standard deviation. Nevertheless, all other variables also do have a considerable variance. For this reason, it does not make sense to cut remove a variable from our algorithm. Especially due to the fact that our data set is relatively small anyways and we do not need to free up computing power.

## 2 Analysis

In this analysis we will always look at three factors. The accuracy, sensitivity and specificity of a model. The accuracy describes how often our model correctly predicts the class (health status) of a patient. The sensitivity describes how many people that are not healthy have been detected, while the specificity describes how many healthy people have correctly been predicted as such. In this example, it is most important than people with abnormalities (sensitivity) are correctly identified as such. However overall accuracy is also very important.

### 2.1 Single Variable
#### 2.1.1 Method

First, we want to try predicting the health status of a patient with only one predictor. For this, we will try finding a value, below or above which we predict a patient to be abnormal. Below, I have included exemplary code for the first predictor. If you want to see the code for all predictors, it is provided in the RMarkdown file.

```{r pelvic incidence}
#create a vector of possible threshold values
pi_vector <- c(50:100)
#create a function that tests decision accuracy for value x
pi_func <- function(x) {
  dec_pi <- ifelse(train_set$pelvic_incidence > x,"Abnormal","Normal")
  mean(dec_pi ==train_set$class)
  }
#apply our vector of possible values to the function
pi_res <- sapply(pi_vector,pi_func)
#save x value that maximizes accuracy
pi_max <- pi_vector[which.max(pi_res)]
#calculate the model with maximum accuracy x value
y_hat_pi <- ifelse(train_set$pelvic_incidence > pi_max,"Abnormal","Normal")
#Creating Confusion Matrix
cm <- confusionMatrix(factor(y_hat_pi),factor(train_set$class))
#Storing ensemble accuracy
acc_pi <- cm$overall["Accuracy"]
#Storing Ensemble Sensitivity and Specificity
ss_pi <- cm$byClass[1:2]

```


```{r pelvic incidence visualisation, echo = FALSE}
##Visualisation
#test prediction vs train set
pi_test <- y_hat_pi ==train_set$class
#convert our test to numeric
pi_test <- as.numeric(pi_test)
#split the data into lists
split <- split(pi_test, ceiling(seq_along(pi_test)/5))
#vector that holds all integer values of length of split
index_split <- c(1:length(split))
#sum up the values in the lists
split_sum <- lapply(index_split, function(i) {
  sum(unlist(split[i]))
})
#create a data frame for visualization
pi_df <- data.frame(index = index_split, correct = unlist(split_sum))
#Create a ggplot
pi_g <- pi_df %>% ggplot(aes(x = index, y=correct, fill= correct)) + geom_bar(stat= "identity") + scale_fill_gradient(low = "red",  high = "green",  space = "Lab",  guide = "colourbar",
  aesthetics = "fill") +  labs(title = "pelvic incidence")

```

```{r pelvic tilt, echo = FALSE}
#create a vector of possible threshold values
pt_vector <- c(10:50)

#create a function that tests decision accuracy for value x
pt_func <- function(x) {
  dec_pt <- ifelse(train_set$`pelvic_tilt numeric` > x,"Abnormal","Normal")
  mean(dec_pt ==train_set$class)
  }
#apply our vector of possible values to the function
pt_res <- sapply(pt_vector,pt_func)
#save x value that maximizes accuracy
pt_max <- pt_vector[which.max(pt_res)]
#calculate the model with maximum accuracy x value
y_hat_pt <- ifelse(train_set$`pelvic_tilt numeric` > pt_max,"Abnormal","Normal")
#Creating Confusion Matrix
cm <- confusionMatrix(factor(y_hat_pt),factor(train_set$class))
#Storing ensemble accuracy
acc_pt <- cm$overall["Accuracy"]
#Storing Ensemble Sensitivity and Specificity
ss_pt <- cm$byClass[1:2]
##Visualisation
#test prediction vs train set
pt_test <- y_hat_pt ==train_set$class
#convert our test to numeric
pt_test <- as.numeric(pt_test)
#split the data into lists
split <- split(pt_test, ceiling(seq_along(pt_test)/5))
#vector that holds all integer values of length of split
index_split <- c(1:length(split))
#sum up the values in the lists
split_sum <- lapply(index_split, function(i) {
  sum(unlist(split[i]))
})
#create a data frame for visualization
pt_df <- data.frame(index = index_split, correct = unlist(split_sum))
#Create a ggplot
pt_g <- pt_df %>% ggplot(aes(x = index, y=correct, fill = correct)) + geom_bar(stat= "identity") + scale_fill_gradient(low = "red",  high = "green",  space = "Lab",  guide = "colourbar",
  aesthetics = "fill") + labs(title = "pelvic_tile")


```

```{r lumbar lordosis angle,echo = FALSE}
#create a vector of possible threshold values
ll_vector <- c(25:100)

#create a function that tests decision accuracy for value x
ll_func <- function(x) {
  dec_ll <- ifelse(train_set$lumbar_lordosis_angle > x,"Abnormal","Normal")
  mean(dec_ll ==train_set$class)
  }
#apply our vector of possible values to the function
ll_res <- sapply(ll_vector,ll_func)
#save x value that maximizes accuracy
ll_max <- ll_vector[which.max(ll_res)]
#calculate the model with maximum accuracy x value
y_hat_ll <- ifelse(train_set$lumbar_lordosis_angle > ll_max,"Abnormal","Normal")
#Creating Confusion Matrix
cm <- confusionMatrix(factor(y_hat_ll),factor(train_set$class))
#Storing ensemble accuracy
acc_ll <- cm$overall["Accuracy"]
#Storing Ensemble Sensitivity and Specificity
ss_ll <- cm$byClass[1:2]
##Visualisation
#test prediction vs train set
ll_test <- y_hat_ll ==train_set$class
#convert our test to numericll_test <- as.numeric(ll_test)

#split the data into lists
split <- split(ll_test, ceiling(seq_along(ll_test)/5))
#vector that holds all integer values of length of split
index_split <- c(1:length(split))
#sum up the values in the lists
split_sum <- lapply(index_split, function(i) {
  sum(unlist(split[i]))
})
#create a data frame for visualization
ll_df <- data.frame(index = index_split, correct = unlist(split_sum))
#Create a ggplot
ll_g <- ll_df %>% ggplot(aes(x = index, y=correct, fill = correct)) + geom_bar(stat= "identity") + scale_fill_gradient(low = "red",  high = "green",  space = "Lab",  guide = "colourbar",  aesthetics = "fill") + labs(title = "lumbar_lordosis_angle")
```

```{r sacral slope, echo = FALSE}
#create a vector of possible threshold values
ss_vector <- c(30:125)

#create a function that tests decision accuracy for value x
ss_func <- function(x) {
  dec_ss <- ifelse(train_set$sacral_slope > x,"Abnormal","Normal")
  mean(dec_ss ==train_set$class)
  }
#apply our vector of possible values to the function
ss_res <- sapply(ss_vector,ss_func)
#save x value that maximizes accuracy
ss_max <- ss_vector[which.max(ss_res)]
#calculate the model with maximum accuracy x value
y_hat_ss <- ifelse(train_set$sacral_slope > ss_max,"Abnormal","Normal")
#Creating Confusion Matrix
cm <- confusionMatrix(factor(y_hat_ss),factor(train_set$class))
#Storing ensemble accuracy
acc_ss <- cm$overall["Accuracy"]
#Storing Ensemble Sensitivity and Specificity
ss_ss <- cm$byClass[1:2]
##Visualisation
#test prediction vs train set
ss_test <- y_hat_ss ==train_set$class
#convert our test to numeric
ss_test <- as.numeric(ss_test)
#split the data into lists
split <- split(ss_test, ceiling(seq_along(ss_test)/5))
#vector that holds all integer values of length of split
index_split <- c(1:length(split))
#sum up the values in the lists
split_sum <- lapply(index_split, function(i) {
  sum(unlist(split[i]))
})
#create a data frame for visualization
ss_df <- data.frame(index = index_split, correct = unlist(split_sum))
#Create a ggplot
ss_g <- ss_df %>% ggplot(aes(x = index, y=correct, fill = correct)) + geom_bar(stat= "identity") + scale_fill_gradient(low = "red",  high = "green",  space = "Lab",  guide = "colourbar",
  aesthetics = "fill") + labs(title = "sacral_slope")
```

```{r pelvic radius, echo = FALSE}
#create a vector of possible threshold values
pr_vector <- c(0:125)

#create a function that tests decision accuracy for value x
pr_func <- function(x) {
  dec_pr <- ifelse(train_set$pelvic_radius < x,"Abnormal","Normal")
  mean(dec_pr ==train_set$class)
  }
#apply our vector of possible values to the function
pr_res <- sapply(pr_vector,pr_func)
#save x value that maximizes accuracy
pr_max <- pr_vector[which.max(pr_res)]
#calculate the model with maximum accuracy x value
y_hat_pr <- ifelse(train_set$pelvic_radius < pr_max,"Abnormal","Normal")
#Creating Confusion Matrix
cm <- confusionMatrix(factor(y_hat_pr),factor(train_set$class))
#Storing ensemble accuracy
acc_pr <- cm$overall["Accuracy"]
#Storing Ensemble Sensitivity and Specificity
ss_pr <- cm$byClass[1:2]
##Visualisation
#test prediction vs train set
pr_test <- y_hat_pr ==train_set$class
#convert our test to numeric
pr_test <- as.numeric(pr_test)
#split the data into lists
split <- split(pr_test, ceiling(seq_along(pr_test)/5))
#vector that holds all integer values of length of split
index_split <- c(1:length(split))
#sum up the values in the lists
split_sum <- lapply(index_split, function(i) {
  sum(unlist(split[i]))
})
#create a data frame for visualization
pr_df <- data.frame(index = index_split, correct = unlist(split_sum))
#Create a ggplot
pr_g <- pr_df %>% ggplot(aes(x = index, y=correct, fill = correct)) + geom_bar(stat= "identity") + scale_fill_gradient(low = "red",  high = "green",  space = "Lab",  guide = "colourbar",
  aesthetics = "fill") + labs(title = "pelvic_radius")
```

```{r degree spondylolisthesis, echo = FALSE}
#create a vector of possible threshold values
sp_vector <- c(0:75)
#create a function that tests decision accuracy for value x
sp_func <- function(x) {
  dec_sp <- ifelse(train_set$degree_spondylolisthesis > x,"Abnormal","Normal")
  mean(dec_sp ==train_set$class)
  }
#apply our vector of possible values to the function
sp_res <- sapply(sp_vector,sp_func)
#save x value that maximizes accuracy
sp_max <- sp_vector[which.max(sp_res)]
#calculate the model with maximum accuracy x value
y_hat_sp <- ifelse(train_set$degree_spondylolisthesis > sp_max,"Abnormal","Normal")
#Creating Confusion Matrix
cm <- confusionMatrix(factor(y_hat_sp),factor(train_set$class))
#Storing ensemble accuracy
acc_sp <- cm$overall["Accuracy"]
#Storing Ensemble Sensitivity and Specificity
ss_sp <- cm$byClass[1:2]
##Visualisation
#test prediction vs train set
sp_test <- y_hat_sp ==train_set$class
#convert our test to numeric
sp_test <- as.numeric(sp_test)
#split the data into lists
split <- split(sp_test, ceiling(seq_along(sp_test)/5))
#vector that holds all integer values of length of split
index_split <- c(1:length(split))
#sum up the values in the lists
split_sum <- lapply(index_split, function(i) {
  sum(unlist(split[i]))
})
#create a data frame for visualization
sp_df <- data.frame(index = index_split, correct = unlist(split_sum))
#Create a ggplot
sp_g <- sp_df %>% ggplot(aes(x = index, y=correct, fill = correct)) + geom_bar(stat= "identity") + scale_fill_gradient(low = "red",  high = "green",  space = "Lab",  guide = "colourbar",
  aesthetics = "fill") + labs(title = "degree_spondylolisthesis")

```
#### 2.1.2 Results

The degree of spondylolisthesis was the most powerful single predictor with an accuracy of close to 80% and a sensitivity of 81%. This could've been expected, if you look at the distribution as well as the variance of the variable. While the lumbar lordosis angle had an accuracy of only about 68%, its sensitivity was at close to 96%. 

```{r evaluation of simple models, echo = FALSE}
#Create an accuracy data frame
acc_df_sim = tibble(model = c("Only Pelvic Incidence", "Only Pelvic Tilt", "Only Lumbar Lordosis Angle", "Only Sacral Slope", "Only Pelvic Radius", "Only Degree of Spondylolisthesis"), Accuracy = c(acc_pi,acc_pt,acc_ll, acc_ss, acc_pr, acc_sp), Sensitivity = c(ss_pi[1],ss_pt[1],ss_ll[1],ss_ss[1],ss_pr[1],ss_sp[1]), Specificity = c(ss_pi[2],ss_pt[2],ss_ll[2],ss_ss[2],ss_pr[2],ss_sp[2]))

#Showing Table
acc_df_sim %>% knitr::kable()

```

From the graphs below, we can see the correct predictions a method made in a region of the data set. For the graphs, the dataset was sliced into lists of 5. Here, we can see how many of those 5 predictions were correct within each list. While the index of the list has no relation to the variable value, every list includes a different group of patients of unique values. If different predictors/methods are good or bad at different parts of the graph, it might be worth combining these predictors/methods. 

From the graphs below, we can see that this is the case. For example, the degree_spondylolisthesis variable is very good at predicting the lists in the middle, while pelvic_tie is much better at predicting the lists closer to zero.



```{r plot accuracy of simple models, echo = FALSE}
#Create grid
ggarrange(pi_g,pt_g,ll_g,ss_g,pr_g,sp_g,nrow = 3, ncol=2, common.legend = TRUE, legend = "right")

```

### 2.2 Multivariable Models
#### 2.2.1 Method

Now we will look at multiple predictors. We will use the train function from the caret package to train or models with different methods. We will look at the methods "Random Forest", "knn" and "svmLinear". All three are popular machine learning algorithms. We will start by finding the best tuning parameters for the algorithm, before we run the algorithm with optimised paramters.  

Below, you will find an example code from the random forest method. If you want to see the full code, you can find it in the RMarkdown file.
```{r random forest ranger, warning = FALSE}
#Create a df with tuneGrid options
rf_grid = expand.grid(mtry = seq(1,6,1),splitrule = "extratrees", min.node.size = seq(4,12,2))
#Setting the seed
set.seed(1, sample.kind = "Rounding")
#Find best tuneGrid options for train method
train_rf <- train(class ~ .,
                 method = "ranger",
                 data = train_set,
                tuneGrid = rf_grid)

#Create a df with best tuneGrid options
rf_values <- data.frame(mtry = c(2), splitrule = c("extratrees"),
                        min.node.size = c(8))	
#Setting the seed
set.seed(1, sample.kind = "Rounding")
#Train Model with best variables
rf_fit <- train(class ~ .,
                 method = "ranger",
                 data = train_set,
                tuneGrid = rf_values)
#Predict y_hat
y_hat_rf <- predict(rf_fit, train_set, type = "raw")
#Create confusion matrix
cm <- confusionMatrix(y_hat_rf,
                factor(train_set$class))
#Store accuracy
acc_rf <- cm$overall["Accuracy"]
#Store Sensitivity and specificity
ss_rf <- cm$byClass[1:2]


```

```{r random forest show results of tuneGrid, echo = FALSE}
#plot best tuneGrid options
ggplot(train_rf, highlight = TRUE)
#Print best tune grid options
print(train_rf$bestTune)
```
##### 2.2.1.1 Random Forest  

Ideal parameters are mtry = 2, splitrule = "extratrees" and min.node.size = 8.  
```{r Random Forest Visualisation, echo = FALSE, fig.align = 'center'}
#Plotting
#Test prediction vs train set
rf_test <- y_hat_rf ==train_set$class
#convert our test to numeric
rf_test <- as.numeric(rf_test)

#Split the data into lists
split <- split(rf_test, ceiling(seq_along(rf_test)/5))
#Vector that holds all integer values of length of split
index_split <- c(1:length(split))
#Sum up the values in the lists
split_sum <- lapply(index_split, function(i) {
  sum(unlist(split[i]))
})
#Create a data frame for visualization
rf_df <- data.frame(index = index_split, correct = unlist(split_sum))
#Create a ggplot
rf_g <- rf_df %>% ggplot(aes(x = index, y=correct, fill = correct)) + geom_bar(stat= "identity") + scale_fill_gradient(low = "red",  high = "green",  space = "Lab",  guide = "colourbar",
  aesthetics = "fill") + labs(title = "Random Forest")
```

 
```{r knn, warning = FALSE, echo = FALSE, fig.align = 'center'}
#Create a df with tuneGrid options
knn_tune <- data.frame(k = seq(1, 71, 2))
#Setting the seed
set.seed(1, sample.kind = "Rounding")
#Find best tuneGrid options for train method
knn_fit <- train(class ~ .,
                 method = "knn",
                 data = train_set,
                 tuneGrid = knn_tune)
#plot best tuneGrid options
ggplot(knn_fit, highlight = TRUE)
#Print best tune grid options
print(knn_fit %>% .$bestTune)
#Create a df with best tuneGrid options
knn_value <- data.frame(k = c(27))
#Setting the seed
set.seed(1, sample.kind = "Rounding")
#Train Model with best variables
knn_fit <- train(class ~ .,
                 method = "knn",
                 data = train_set,
                 tuneGrid = knn_value)
#Predict y_hat
y_hat_knn <- predict(knn_fit, train_set)
#Create the confusion matrix
cm <- confusionMatrix(y_hat_knn,factor(train_set$class))
#Store accuracy
acc_knn <- cm$overall["Accuracy"]
#Store sensitivity and specifity
ss_knn <- cm$byClass[1:2]

##Plotting
#test prediction vs train set
knn_test <- y_hat_knn ==train_set$class
#convert our test to numeric
knn_test <- as.numeric(knn_test)

#split the data into lists
split <- split(knn_test, ceiling(seq_along(knn_test)/5))
#vector that holds all integer values of length of split
index_split <- c(1:length(split))
#sum up the values in the lists
split_sum <- lapply(index_split, function(i) {
  sum(unlist(split[i]))
})
#create a data frame for visualization
knn_df <- data.frame(index = index_split, correct = unlist(split_sum))
#Create a ggplot
knn_g <- knn_df %>% ggplot(aes(x = index, y=correct, fill = correct)) + geom_bar(stat= "identity") + scale_fill_gradient(low = "red",  high = "green",  space = "Lab",  guide = "colourbar",
  aesthetics = "fill") + labs(title = "knn")


```
##### 2.2.1.2 knn    

Ideal parameter is k = 27. 

```{r svmLinear, warning = FALSE, echo = FALSE, fig.align = 'center'}
#Create a df with tuneGrid options
svm_grid = expand.grid(C = seq(1,14,1))
#Setting the seed
set.seed(1, sample.kind = "Rounding")
#Find best tuneGrid options for train method
train_svm <- train(class ~ .,
                 method = "svmLinear",
                 data = train_set,
                 tuneGrid = svm_grid)
#plot best tuneGrid options
ggplot(train_svm, highlight = TRUE)
#Print best tune grid options
print(train_svm$bestTune)
#Create a df with best tuneGrid options
svm_values <- data.frame(C = c(4))
#Setting the seed
set.seed(1, sample.kind = "Rounding")
#Train Model with best variables
svm_fit <- train(class ~ .,
                 method = "svmLinear",
                 data = train_set,
                tuneGrid = svm_values)
#Predict y_hat
y_hat_svm <- predict(svm_fit, train_set, type = "raw")
#Creaate confusion matrix
cm <- confusionMatrix(y_hat_svm,factor(train_set$class))
#Store accuracy
acc_svm <- cm$overall["Accuracy"]
#Store sensitivity and specificity
ss_svm <- cm$byClass[1:2]
#Plotting
#test prediction vs train set
svm_test <- y_hat_svm ==train_set$class
#convert our test to numeric
svm_test <- as.numeric(svm_test)

#split the data into lists
split <- split(svm_test, ceiling(seq_along(svm_test)/5))
#vector that holds all integer values of length of split
index_split <- c(1:length(split))
#sum up the values in the lists
split_sum <- lapply(index_split, function(i) {
  sum(unlist(split[i]))
})
#create a data frame for visualization
svm_df <- data.frame(index = index_split, correct = unlist(split_sum))
#Create a ggplot
svm_g <- svm_df %>% ggplot(aes(x = index, y=correct, fill = correct)) + geom_bar(stat= "identity") + scale_fill_gradient(low = "red",  high = "green",  space = "Lab",  guide = "colourbar",
  aesthetics = "fill") + labs(title = "svmLinear")

```
##### 2.2.1.3 svmLinear  

Ideal parameter is C = 4.  

## 2.2.2 Results

We can see that the Random Forest model has performed the best out of the three with an accuracy of 96% and a sensitivity of 94%, while the specificity is also quiet high at 90%. Nevertheless, the other two models performed better than all of the single variable predictions.  
```{r evaluation of train methods, echo = FALSE}
#Create a new tibble for the train models
acc_df_train <- tibble(model = c("Random Forest", "knn", "svmLinear"), Accuracy = c(acc_rf,acc_knn,acc_svm), Sensitivity = c(ss_rf[1],ss_knn[1],ss_svm[1]), Specificity = c(ss_rf[2],ss_knn[2],ss_svm[2]))
#Print the new tibble
acc_df_train %>% knitr::kable()
```


Looking at our graphs, we can see that Random Forest perfectly predicted almost all lists. However, we might be able to slightly improve upon that by building an ensemble of the three methods.  
```{r plot accuracy train models, echo = FALSE}
#Create Grid
ggarrange(rf_g,knn_g,svm_g,nrow = 2, ncol=2, common.legend = TRUE, legend = "bottom")

```

## 2.3 Ensemble

Below, you can find the code for a simple ensemble of the three training models. It always assigns the health status, of which the majority of models are in favour.  
```{r Ensemble, echo = TRUE}
#Storing the y_hats
y_hat_stored <- cbind(y_hat_knn,y_hat_rf,y_hat_svm)
#Creating average prediction
y_hat_means <- rowMeans(y_hat_stored == "1")
#Creating ensemble y_hat
y_hat_ensemble <- ifelse(y_hat_means > 0.5,"Abnormal","Normal")
#Creating Confusion Matrix
cm <- confusionMatrix(factor(y_hat_ensemble),factor(train_set$class))
#Storing ensemble accuracy
acc_ensemble <- cm$overall["Accuracy"]
#Storing Ensemble Sensitivity and Specificity
ss_ensemble <- cm$byClass[1:2]


```


We can see that the ensemble model could not improve upon the Random Forest model, with an accuracy of 89% and a sensitivity of about 93%.  


```{r Ensemble Results, echo = FALSE}
#Saving Ensemble Tibble
acc_df_ensemble <- tibble(model = c("Train Ensemble"),Accuracy = c(acc_ensemble), Sensitivity = c(ss_ensemble[1]), Specificity = c(ss_ensemble[2]))
#Printing Tibble
acc_df_ensemble %>% knitr::kable()
```


## 2.4 Choosing our Method 

Looking at all our models, we can clearly see that the Random Forest model has performed the best out of all models. The next closest was the ensemble model, with the pelvic incidence proving to be the worst predictor, judging by sensitivity.  
```{r All evaluations, echo= FALSE}
#Summarising all models
all_tbl <- acc_df_sim
all_tbl <-all_tbl %>% add_row(acc_df_train) %>% add_row(acc_df_ensemble)
#Printing Tibble
all_tbl %>% knitr::kable()
```

## 3 Result

Now, we will run our model on the test set. Below, you find the code for our final model.  
```{r Final Model, warning = FALSE}
##1. RF Model

#Create a df with best tuneGrid options
rf_values <- data.frame(mtry = c(2), splitrule = c("extratrees"),
                        min.node.size = c(8))
#Setting the seed
set.seed(1, sample.kind = "Rounding")
#Train Model with best variables
rf_fit <- train(class ~ .,
                 method = "ranger",
                 data = train_set,
                tuneGrid = rf_values)
#Predict y_hat
y_hat_rf_final <- predict(rf_fit, test_set, type = "raw")
#Creating Confusion Matrix
cm <- confusionMatrix(y_hat_rf_final,factor(test_set$class))
#Storing ensemble accuracy
acc_final <- cm$overall["Accuracy"]
#Storing Ensemble Sensitivity and Specificity
ss_final <- cm$byClass[1:2]
```

We can see that our final model performed at an accuracy of 87.1 % and a sensitivity of 95%. This means that out of 20 patients with abnormalities regarding their spine, 19 are correctly identified. With a specificiy of 0.7, there is a considerable amount of "false positives". However, this is okay in terms of this model, as we wanted to make sure that we would identified all patients with a problem.  
```{r Printing Final Model, echo = FALSE}
#Creating Tibble
final_df <- tibble(model = c("Final Model"), Accuracy = c(acc_final), Sensitivity = c(ss_final[1]), Specificity = c(ss_final[2]))
#Printing Tibble
final_df %>% knitr::kable()
```

## 4 Conclusion

During this report, I managed to create a prediction method that correctly identifies patients with spine abnormalities 95% of the time. This is a rate with which I am satisfied. However, the specificity is quiet low at 70%. The limiting factor of this analysis quiet clearly was the relatively small dataset, which forced me to decide between the two. Overall the accuracy of 87.1% is also good. In the future, I might try more machine learning models on this data set and implement a better working ensemble. Furthermore, I trained my models aiming for highest accuracy. Alternatively, I could've also trained for better Sensitivity. Nevertheless, it was an amazing first project and I can't wait to build upon it.  
