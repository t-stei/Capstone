---
title: "MovieLens Recommendation System"
author: "Til Stein"
date: "16/05/2020"
output: pdf_document
---

```{r Package installation, include=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(ggpubr)) install.packages("ggpubr", repos = "http://cran.us.r-project.org")
```

```{r Script to download data, include = FALSE}
################################
# Create edx set, validation set
################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
 # https://grouplens.org/datasets/movielens/10m/
 # http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
 download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
 colnames(movies) <- c("movieId", "title", "genres")
 movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                            title = as.character(title),
                                            genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
 edx <- movielens[-test_index,]
 temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
      semi_join(edx, by = "movieId") %>%
      semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
 edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

```{r README, include = FALSE}
####
# I disabled warnings for all chunks where I applied the sample.kind = "Rounding" argument to the set.seed function. Feel free to activate it, if you want to check for warnings.

# This project was written on R 3.6.3
####
```

```{r Converting timestamp to date, include = FALSE}
#Here we add an extra column containing the timestamp variable in a date format
edx <- mutate(edx, date=as_datetime(timestamp))
#Mutating the date variable to week, so it is easier for further analyzing
edx <- mutate(edx, date = round_date(date, unit = "week"))
```

## 1. Introduction

In this analysis, I will try to predict the ratings users give to movies, based on the edx database. I will adopt a linear model including the predictors userId, movieId, genres and week. Our Measure of success is the root mean squared error (RMSE), which describes the difference between our prediction and the actual rating.

## 1.1 Data Source

The data is provided as part of the HarvardX Data Science: Capstone Course. You will find the script at the beginning of the RMarkdown document. It contains selected data from the popular Movielens dataset.

## 1.2 Data Splitting
```{r Partitioning the data, echo = FALSE, warning = FALSE}
#Setting the Seed
set.seed(1, sample.kind = "Rounding") #as I am using R 3.6 or later

#Now I will split the edx data into a training and a test set.
#I will train my variables with the train_set and use the test_set for verification

#Creating my Data Partition Index
test_index <- createDataPartition(edx$rating, times = 1, p = 0.2, list = FALSE)
#Creating the Test Data
test_set <- edx[test_index,]
#Creating the Training Data
train_set <- edx[-test_index,]

#remove users, movies and genres that appear in the test set but do not appear in the training set
test_set <- test_set %>%
  semi_join(train_set, by= "movieId") %>%
  semi_join(train_set, by= "userId") %>%
  semi_join(train_set, by= "genres") %>%
  semi_join(train_set, by="date")
```
For this analysis, I randomly partitioned the data into a training set and a test set. I will use the training set to build my algorith and the test set to improve it throughout the analysis. 90% of the data is in the training set, while 10% are in the test set. This is a very common way of splitting the data, as it gives me a good amount of entries to train my algorithm, as well as sufficient patients to test my algorithm. I have removed all entries from the test_set that have no matching entries in the train_set. Furthermore, we have a validation data set, which we will use only at the end, to test our algorithm.

### 1.3 Data Insight

The data set holds six variables with 9000055 observations each.  

```{r dim edX, echo = FALSE}
#This will show the Dimensions of our database edx
dim(edx)
```
As this is quiet large, I created an even smaller data set called insight_data with only 10% of the train_set data. This makes it much faster to visualise the dataset, without loosing much information.

```{r, echo = FALSE, warnings = FALSE}
#Setting the seed
set.seed(1, sample.kind = "Rounding")
#Creating Data Partitioning index
insight_index <- createDataPartition(train_set$rating, times = 1, p=0.1, list=FALSE)
#Creating smaller dataframe
insight_data <- edx[insight_index, ]
```


#### 1.3.1 Prediction Variable

We will try to predict the variable called "rating" from the remaining variables (predictors). The rating variable can take values between 0.5 and 5, with steps of 0.5. It is given to the movies by the users. 

```{r unique ratings, echo = FALSE}
print("Unique Ratings:")
sort(unique(insight_data$rating))
```

First, we want to take a look at the distribution of our ratings variable, that we want to predict. From the plot we can see, that the integer ratings are much more common, than the half-star ratings. We can also see that "3" and "4" are the most common ratings.  
```{r Rating Distribution, echo = FALSE, warnings = FALSE}
insight_data %>% ggplot(aes(x = rating, fill= "blue")) +
  geom_bar() + theme(legend.position = "null")
```

#### 1.3.2 Predictors

Let's briefly talk about the predictors. First, there is the UserID. It is an integer number unique to each user. Then, there are the variables movieID and title. These contain the same information. However, one in movieID, the information is encoded as an integer, while in title it is encoded as a character string. Then, there is the genres variable. It is a string and can hold multiple genres of a movie. For example, the genre can be only "Comedy" or "Romance", but also "Comedy | Romance", which stand for "Comedy" and "Romance". Lastly, we have a timestamp for each rating. It is originally stored as an integer, but I also created the date variable in the format date. 

```{r Showing Classes, echo = FALSE}
#Create classes
class_ui <- class(edx$userId)
class_mi <- class(edx$movieId)
class_t <- class(edx$title)
class_g <- class(edx$genres)
class_ti <- class(edx$timestamp)
class_d <- class(edx$date)
#Create a Dataframe with classes
classes <- tibble(Variable = c("UserId", "MovieId","Title","Genres","Timestamp","Date"), 
                  Class =  c(class_ui,class_mi,class_t,class_g,class_ti,class_d[1]))
#Displaying df
classes %>% knitr::kable()


```

In the following, I have created graphs that show correlations between the rating a movie is given and a single predictor.  

```{r Movie Effect, echo = FALSE}
#clearing the temp variable
temp <- NULL
#1. creating temporary mean_rating by movie
temp <- insight_data %>% group_by(movieId) %>%
  summarize(mean_rating = mean(rating), a = (movieId[1]))
#2. Factoring the movie, so we can display it in ascending order
temp$a <- factor(temp$a, 
                        levels = temp$a[order(temp$mean_rating)])

#3. Save graph that shows mean rating by ascending order in relation to the movie
temp %>% ggplot(aes(x = a, y = mean_rating, fill = mean_rating, group = 1)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_blank()) +
  xlab("Movie") +
  labs(title = "Mean Rating by Movie ID") +
  scale_fill_gradient(low = "red",
  high = "green",
  space = "Lab",
  na.value = "grey50",
  guide = "colourbar",
  aesthetics = "fill"
)
```

```{r Genre Effect, echo = FALSE}
#Genre Effect
#clearing the temp variable
temp <- NULL
#1. creating temporary mean_rating by genre
temp <- insight_data %>% group_by(genres) %>%
  summarize(mean_rating = mean(rating), a = (genres[1]))

#2. Factoring the genre, so we can display it in ascending order
temp$a <- factor(temp$a, 
                        levels = temp$a[order(temp$mean_rating)])

#3. Save graph that shows mean rating by ascending order in relation to the genre
temp %>% ggplot(aes(x = a, y = mean_rating, fill = mean_rating, group = 1)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_blank()) +
  xlab("Genre") +
  labs(title = "Mean Rating by Genre") +
  scale_fill_gradient(low = "red",
  high = "green",
  space = "Lab",
  na.value = "grey50",
  guide = "colourbar",
  aesthetics = "fill"
)
```

```{r User Effect, echo = FALSE}
#clearing the temp variable
temp <- NULL
#1. creating temporary mean_rating by user
temp <- insight_data %>% group_by(userId) %>%
  summarize(mean_rating = mean(rating), a = (userId[1]))

#2. Factoring the useres, so we can display it in ascending order
temp$a <- factor(temp$a, 
                        levels = temp$a[order(temp$mean_rating)])

#3. Save graph that shows mean rating by ascending order in relation to the user
temp %>% ggplot(aes(x = a, y = mean_rating, fill = mean_rating, group = 1)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_blank()) +
  xlab("User") +
  labs(title = "Mean Rating by User ID") +
  scale_fill_gradient(low = "red",
  high = "green",
  space = "Lab",
  na.value = "grey50",
  guide = "colourbar",
  aesthetics = "fill"
)

```

```{r Time Effect, echo = FALSE}
#clearing the temp variable
temp <- NULL

#Here we will plot mean rating by weeks, to see if the week that people rated a movie had an effect on the movie. We use the round_date function to return the weeks. We will do the same with month and year.
insight_data %>% mutate(date = round_date(date, unit = "week")) %>%
	group_by(date) %>%
	summarize(rating = mean(rating)) %>%
	ggplot(aes(date, rating)) +
	geom_point() +
	geom_smooth() +
  labs(title = "Mean Rating by week")
```

As we can see, all variables have an influence on the mean rating. The effect is especially strong for movieId and UserId. The week a user rated a movie is much less of an influence. Our findings are very intuitive. For example, some movies are generally seen as better than others and therefore rated better. Furthermore, some people generally seem to be more generous when rating movies than others.

## 2. Analysis

Now we will start our prediction. I will use a linear model that predicts y_hat by looking at mu (mean rating) and the variance from mu inserted by the movie, user, genre and date effect. As mentioned in the introduction, we will use the RMSE as our measure of predction success. The function is as follows: 
```{r RMSE function, echo = TRUE}
#Create the RMSE function
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))}
```

### 2.1 Guessing

As a first comparison, we will try to simply guess the movie ratings. This will give us a reference RMSE value. I have included the code, so you can see how our guesses were generated. 

```{r Analysis Part 1 Guessing, echo = TRUE, warnings = FALSE}
#Guessing
set.seed(1, sample.kind= "Rounding")

#Creating a vector of all possible outcomes
rating_outcomes <- c(0.5,1,1.5,2,2.5,3,3.5,4,4.5,5)

#The number of times we want to replicate the random "drawing" of ratings
n=10

#Setting the seed
set.seed(1, sample.kind= "Rounding")
#With this function, we replicate a random drawing of values from our vector random_drawing vector n times and calulate the RMSE of each drawing
results_guessing_rmse <- replicate(n,{
y_hat <- sample(rating_outcomes, length(test_set$rating), replace=TRUE)
RMSE(test_set$rating,y_hat)
})
#Getting the average RMSE of our n drawings
guessing_rmse <- mean(results_guessing_rmse)

```

```{r Analysis Part 1 Guessing storing variables, echo = FALSE}
#adding our rmse to the rmse_results dataframe
rmse_results <- tibble(method = "Guessing", RMSE = guessing_rmse)
#Printing our RMSE
rmse_results %>% knitr::kable()
```

We can see that our RMSE lies at 1.9408. This means than on average, we are about 2 ratings of the true rating. Let's see if we can improve on that.

### 2.2 Using the Mean  
```{r Analysis Part 2 Mean, echo = TRUE}
#getting mu
mu <- mean(train_set$rating)
#calculating our naive_rmse by comparing true rating with our mean
naive_rmse <- RMSE(test_set$rating,mu)
#adding our naive_rmse to the rmse_results dataframe
rmse_results <- bind_rows(rmse_results,
                          tibble(method ="Mean",
                                     RMSE = naive_rmse))
#Printing our RMSE
rmse_results[2,] %>% knitr::kable()

```

Now we start with the real analysis. We will try predicting the ratings by always guessing the mean rating. The code explains how it is done. We can see that our RMSE has significantly improved from just guessing, but is still quiet high at about 1.0599.  

### 2.3 Including MovieId
From now on, I only include the code that shows the development of our code and not the calculation and printing of RMSE.  
```{r Analysis Part 3 MovieId, echo = TRUE}
#Creating our b_i variable by looking at the difference between rating and mu
movie_avgs <- train_set %>% 
     group_by(movieId) %>% 
     summarize(b_i = mean(rating - mu))
#calculating our y_hat(predicted ratings) with our new model
predicted_ratings <- mu + test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     .$b_i

```

```{r Analysis Part 3 MovieID Print, echo = FALSE}
#calculating rmse of our new model
model_1_rmse <- RMSE(predicted_ratings, test_set$rating)
#adding our results to the rmse_results dataframe
rmse_results <- bind_rows(rmse_results,
                          tibble(method ="Movie Effect Model",
                                    RMSE = model_1_rmse))
#Print RMSE
rmse_results[3,] %>% knitr::kable()
```

This time, we add the variation that is beeing explained by the movie. By including this, we achieve a significant improvement of our RMSE, which is now at 0.9437.   

### 2.4 Including UserId

```{r Analysis Part 4 UserId, echo = TRUE}
#Creating our b_u variable by looking at the variance not explained by b_i
user_avgs <- train_set %>%
  left_join(movie_avgs, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
#calculating our y_hat(predicted ratings) with our new model
predicted_ratings <- test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     left_join(user_avgs, by='userId') %>%
     mutate(pred = mu + b_i + b_u) %>%
     .$pred
```

```{r Analysis Part 4 UserId Print, echo = FALSE}
#calculating rmse of our new model
model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
#Adding our results to the rmse_results dataframe
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_2_rmse ))
#Printing RMSE
rmse_results[4,] %>% knitr::kable()
```

In our introduction we also saw that the user had a significant effect on the rating. With the code above, we take this into consideration. Our RMSE has improved again and now stands at 0.8659.

### 2.5 Including Genres
```{r Analysis Part 5 GenreAverages, echo = TRUE}
#Creating our b_g variable by looking at the variance not explained by b_i or b_u
genre_avgs <- train_set %>%
  left_join(movie_avgs, by="movieId") %>%
  left_join(user_avgs, by="userId") %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u))
#calculating our y_hat(predicted ratings) with our new model
predicted_ratings <- test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     left_join(user_avgs, by='userId') %>%
     left_join(genre_avgs, by='genres') %>%
     mutate(pred = mu + b_i + b_u + b_g) %>%
     .$pred
```

```{r Analysis Part 5 GenreAverages Print, echo = FALSE}
#calculating rmse of our new model    
model_3_rmse <- RMSE(predicted_ratings, test_set$rating)
#Adding our results to the rmse_results dataframe
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Movie + User + Genre Effects Model",  
                                     RMSE = model_3_rmse ))
#Printing our RMSE
rmse_results[5,] %>% knitr::kable()
```

Now, we have also added the genre as a variable. Our RMSE is quiet low at 0.8655.

### 2.6 Including Weekly Effect
```{r Analysis Part 6 Weekly Effect, echo = TRUE, warnings = FALSE}
#Creating our b_d variable by looking at the variance not explained by b_i, b_u or b_g
date_avgs <- train_set %>%
  left_join(movie_avgs, by="movieId") %>%
  left_join(user_avgs, by="userId") %>%
  left_join(genre_avgs, by="genres") %>%
  group_by(date) %>%
  summarize(b_d = mean(rating - mu - b_i - b_u - b_g))
#calculating our y_hat(predicted ratings) with our new model
predicted_ratings <- test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     left_join(user_avgs, by='userId') %>%
     left_join(genre_avgs, by='genres') %>%
     left_join(date_avgs, by="date") %>%
     mutate(pred = mu + b_i + b_u + b_g + b_d) %>%
     .$pred
```

```{r Analysis Part 6 Weekly Effect Print, echo = FALSE}
#calculating rmse of our new model 
model_4_rmse <- RMSE(predicted_ratings, test_set$rating)
#Adding our results to the rmse_results dataframe
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Movie + User + Genre +Date Effects Model",  
                                     RMSE = model_4_rmse ))
#Printing our RMSE
rmse_results[6,] %>% knitr::kable()
```

To achieve the final improvement of our RMSE, we include the weekly variation in RMSE. Our RMSE has improved ever so slightly to 0.8654.

## 3. Results

In this section, we will look at the performance of our algorithm with an unknown data set. I will train our variables mu, b_i, b_u, b_g and b_d from the entire edx data set. Furthermore, I will replace NA Values with 0. Lastly, I predict the validation$ratings.

## 3.1 Final Model

In the following, you can see the code for our final model.
```{r Final Model, echo = TRUE}
#Creating Mu from entire edx data set
mu <- mean(edx$rating)
#Creating b_i from entire edx data set
movie_avgs <- edx %>% group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))
#Creating b_u from entire edx data set
user_avgs <- edx %>% 
  left_join(movie_avgs, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating- b_i -mu))
#Creating b_g from entire edx data set
genre_avgs <- edx %>%
  left_join(movie_avgs, by="movieId") %>%
  left_join(user_avgs, by="userId") %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u))
#Creating b_d from entire edx data set
date_avgs <- edx %>%
  left_join(movie_avgs, by="movieId") %>%
  left_join(user_avgs, by="userId") %>%
  left_join(genre_avgs, by="genres") %>%
  group_by(date) %>%
  summarize(b_d = mean(rating - mu - b_i - b_u - b_g))

#Adding the b_i, b_u and b_g variables to the validation data frame
validation <- validation %>%
  mutate(date=as_datetime(timestamp)) %>%
  mutate(date = round_date(date, unit = "week")) %>%
  left_join(movie_avgs, by="movieId") %>%
  left_join(user_avgs, by="userId") %>%
  left_join(genre_avgs, by="genres") %>%
  left_join(date_avgs, by="date")

#Replacing NA Values in validation data frame
validation$b_i[is.na(validation$b_i)] <- 0
validation$b_u[is.na(validation$b_u)] <- 0
validation$b_g[is.na(validation$b_g)] <- 0
validation$b_d[is.na(validation$b_d)] <- 0

#Doing our Prediction
prediction <- validation %>%
  mutate(pred = mu + b_i + b_u + b_g + b_d) %>%
  .$pred
```

## 3.2 Discussing our results

```{r Final Model Calculating and Printing RMSE, echo = FALSE}

#Calculating RMSE
final_RMSE <- RMSE(validation$rating, prediction)

#Adding our final RMSE to the table
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Final Result",  
                                     RMSE = final_RMSE ))
#Printing our RMSE
rmse_results[7,] %>% knitr::kable()

```

As we can see, our final RMSE is at 0.8648392. If we compare it with the table below, which shows the development of our RMSE throughout the models, we can see that it is performing slightly better with the unknown validation set, than it did with the training set. This can be down to many factors. It could show that we did not overtrain our algorithm, but it could also be down to luck. Nevertheless, I am satisfied with the result.

```{r Comparing results, echo=FALSE}
#Showing RMSE Results Table
rmse_results %>% knitr::kable()
```

## 4 Conclusion

By slowly building a linear regression analysis, we managed to obtain an RMSE of 0.8648392. This is a results I am quiet satisfied with. However, it could propably be improved, if we would look closer at the genres, e.g. counting "Comedy | Romance" as two genres. We could also try to comvine this linear regression model with other machine learning algorithms. Unfortunately, my computer was not strong enough to do such calculations. It was a great project and a great introduction of the field of data analytics.